{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh7ucARYpvXO"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ],
      "id": "vh7ucARYpvXO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-TXM6XXpvXQ",
        "outputId": "2d847296-e289-408f-d254-43e8f12cfc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "6-TXM6XXpvXQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Force redownload of punkt in case of corruption\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords', force=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "330KV7vhsRx-",
        "outputId": "7c9f6d69-cb25-46b7-d9d1-fb8e26bfdabb"
      },
      "id": "330KV7vhsRx-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUjboHpqpvXR"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ],
      "id": "AUjboHpqpvXR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfkrcVVlpvXS",
        "outputId": "bafb8611-ab9c-4744-df68-c4958c88b8c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Ensure stopwords are available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load models\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase and tokenize with spaCy\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    cleaned_tokens = [token.text for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Print cleaned tokens\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(cleaned_tokens)\n"
      ],
      "id": "rfkrcVVlpvXS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM8YGhKOpvXS"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ],
      "id": "LM8YGhKOpvXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reAYQxpjpvXS",
        "outputId": "605c412c-a20e-4874-a90c-d1bef124ad54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n",
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'combines'), ('combines', 'linguistics'), ('linguistics', 'computer'), ('computer', 'science')]\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text.lower())\n",
        "    cleaned_tokens = [token.text for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Clean the text\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(\"Cleaned Tokens:\", cleaned_tokens)\n",
        "\n",
        "# Generate bigrams from cleaned tokens\n",
        "bigrams = list(ngrams(cleaned_tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "id": "reAYQxpjpvXS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUsxJ6udpvXS"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ],
      "id": "kUsxJ6udpvXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy0Vx7QdpvXT",
        "outputId": "8a3b6a16-11cf-4c87-a4f3-f2b5ffc3a86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama PERSON\n",
            "Hawaii GPE\n",
            "2008 DATE\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
        "doc = nlp(sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "id": "dy0Vx7QdpvXT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SndFna3wpvXT"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ],
      "id": "SndFna3wpvXT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-lK4GL0pvXT",
        "outputId": "ea90047e-a712-4fe6-d9ee-5680cb1a1b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer Output:\n",
            " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "TF-IDF Vectorizer Output:\n",
            " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
            "  0.57735027 0.         0.         0.         0.         0.        ]\n",
            " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
            "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
            " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5628291 ]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
      ],
      "id": "X-lK4GL0pvXT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KbgbGZ9pvXT"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ],
      "id": "1KbgbGZ9pvXT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO_torzTpvXU",
        "outputId": "31e6a197-3328-423e-a152-9e7d2f96bad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine':\n",
            " [-0.7506131  -0.57648134  0.64351434  0.34771815  0.45008683 -0.31984502\n",
            "  1.3374304   0.6823808  -0.2497879   0.01502723  0.20069116 -0.53005815\n",
            " -0.32142693  0.6083895   0.5911227   1.3969526  -1.3394687  -0.49667895\n",
            "  0.93146133  0.76211375 -0.63203835  1.1820786  -0.8377956   0.02632815\n",
            " -0.2938518   0.6069318   1.5544686  -0.04658484 -0.4521918   0.48126173\n",
            "  0.02117339  0.9538507   0.38607836  0.03060612 -1.2614324  -0.71200204\n",
            " -0.0582068   0.9979756   0.39407492  0.03983042 -0.9099759  -0.30680424\n",
            "  0.81676024  0.40132928 -0.6591901  -0.52833277 -0.1020698  -0.39648485\n",
            " -0.2746659  -0.5868281   0.11670423 -0.02715784  0.10342616 -0.71523666\n",
            "  0.78196347  0.26182324  1.2007883   0.40819865 -0.81201667  0.10142356\n",
            " -0.92464274 -0.0610747  -0.28506368 -0.27212036 -0.06658131  0.21739644\n",
            " -0.34570417 -0.7191127  -0.6493219  -0.07578599 -0.26895642  0.25262332\n",
            "  0.8506174   0.5730195  -0.10925089 -0.48964912 -0.33062595 -0.6904906\n",
            "  0.58739054  0.52517235 -0.47667012 -0.37759903 -0.88804555 -0.58199006\n",
            "  0.22513609  1.8321126  -0.54407513  0.07091011 -1.0318854   0.15322405\n",
            " -0.44268894  0.00544617  0.5723448   0.2448978  -0.8239452   0.42069194]\n"
          ]
        }
      ],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "# !python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example word vector\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)"
      ],
      "id": "vO_torzTpvXU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}