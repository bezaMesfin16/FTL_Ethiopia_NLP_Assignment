{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vh7ucARYpvXO",
      "metadata": {
        "id": "vh7ucARYpvXO"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d7169caf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (3.8.6)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.9.4-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (2.2.5)\n",
            "Requirement already satisfied: pandas in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: click in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (0.15.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (78.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.58.0-cp310-cp310-win_amd64.whl.metadata (106 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\beza\\anaconda3\\envs\\medibot\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
            "Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl (8.1 MB)\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/8.1 MB 1.1 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 0.8/8.1 MB 1.2 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.0/8.1 MB 1.2 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 1.6/8.1 MB 1.3 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 1.8/8.1 MB 1.3 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 2.1/8.1 MB 1.4 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 2.6/8.1 MB 1.5 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 2.9/8.1 MB 1.5 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 3.4/8.1 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 3.9/8.1 MB 1.6 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 4.2/8.1 MB 1.6 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 4.7/8.1 MB 1.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 5.5/8.1 MB 1.8 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 6.0/8.1 MB 1.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 6.6/8.1 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.3/8.1 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  7.9/8.1 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.1/8.1 MB 2.1 MB/s eta 0:00:00\n",
            "Downloading wordcloud-1.9.4-cp310-cp310-win_amd64.whl (299 kB)\n",
            "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.58.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 0.5/2.2 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.3/2.2 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.1/2.2 MB 3.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 3.2 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
            "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib, wordcloud\n",
            "\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   ------ --------------------------------- 1/6 [fonttools]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   -------------------------- ------------- 4/6 [matplotlib]\n",
            "   --------------------------------- ------ 5/6 [wordcloud]\n",
            "   ---------------------------------------- 6/6 [wordcloud]\n",
            "\n",
            "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 wordcloud-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy matplotlib wordcloud numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6-TXM6XXpvXQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-TXM6XXpvXQ",
        "outputId": "2d847296-e289-408f-d254-43e8f12cfc9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "330KV7vhsRx-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "330KV7vhsRx-",
        "outputId": "7c9f6d69-cb25-46b7-d9d1-fb8e26bfdabb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Force redownload of punkt in case of corruption\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords', force=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AUjboHpqpvXR",
      "metadata": {
        "id": "AUjboHpqpvXR"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67cde0f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Beza\\anaconda3\\envs\\medibot\\lib\\runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Beza\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "rfkrcVVlpvXS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfkrcVVlpvXS",
        "outputId": "bafb8611-ab9c-4744-df68-c4958c88b8c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK resources: {e}\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove punctuation and stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
        "    \n",
        "    return cleaned_tokens\n",
        "\n",
        "# Print cleaned tokens\n",
        "try:\n",
        "    cleaned_tokens = preprocess(text)\n",
        "    print(cleaned_tokens)\n",
        "except LookupError as e:\n",
        "    print(f\"LookupError: {e}\")\n",
        "    print(\"Attempting to download required NLTK resources...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    cleaned_tokens = preprocess(text)\n",
        "    print(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LM8YGhKOpvXS",
      "metadata": {
        "id": "LM8YGhKOpvXS"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reAYQxpjpvXS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reAYQxpjpvXS",
        "outputId": "605c412c-a20e-4874-a90c-d1bef124ad54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Tokens: ['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n",
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'combines'), ('combines', 'linguistics'), ('linguistics', 'computer'), ('computer', 'science')]\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text.lower())\n",
        "    cleaned_tokens = [token.text for token in doc if token.is_alpha and token.text not in stop_words]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Clean the text\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(\"Cleaned Tokens:\", cleaned_tokens)\n",
        "\n",
        "# Generate bigrams from cleaned tokens\n",
        "bigrams = list(ngrams(cleaned_tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kUsxJ6udpvXS",
      "metadata": {
        "id": "kUsxJ6udpvXS"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dy0Vx7QdpvXT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy0Vx7QdpvXT",
        "outputId": "8a3b6a16-11cf-4c87-a4f3-f2b5ffc3a86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama PERSON\n",
            "Hawaii GPE\n",
            "2008 DATE\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
        "doc = nlp(sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SndFna3wpvXT",
      "metadata": {
        "id": "SndFna3wpvXT"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-lK4GL0pvXT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-lK4GL0pvXT",
        "outputId": "ea90047e-a712-4fe6-d9ee-5680cb1a1b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count Vectorizer Output:\n",
            " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "TF-IDF Vectorizer Output:\n",
            " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
            "  0.57735027 0.         0.         0.         0.         0.        ]\n",
            " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
            "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
            " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5628291 ]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1KbgbGZ9pvXT",
      "metadata": {
        "id": "1KbgbGZ9pvXT"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "vO_torzTpvXU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO_torzTpvXU",
        "outputId": "31e6a197-3328-423e-a152-9e7d2f96bad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.5/33.5 MB 1.5 MB/s eta 0:00:22\n",
            "     - -------------------------------------- 1.3/33.5 MB 2.6 MB/s eta 0:00:13\n",
            "     -- ------------------------------------- 2.1/33.5 MB 3.0 MB/s eta 0:00:11\n",
            "     --- ------------------------------------ 2.9/33.5 MB 3.3 MB/s eta 0:00:10\n",
            "     ---- ----------------------------------- 3.9/33.5 MB 3.5 MB/s eta 0:00:09\n",
            "     ----- ---------------------------------- 4.7/33.5 MB 3.6 MB/s eta 0:00:09\n",
            "     ------ --------------------------------- 5.5/33.5 MB 3.6 MB/s eta 0:00:08\n",
            "     ------- -------------------------------- 6.6/33.5 MB 3.7 MB/s eta 0:00:08\n",
            "     -------- ------------------------------- 7.3/33.5 MB 3.8 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 7.9/33.5 MB 3.9 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 7.9/33.5 MB 3.9 MB/s eta 0:00:07\n",
            "     ----------- ---------------------------- 10.0/33.5 MB 3.8 MB/s eta 0:00:07\n",
            "     ------------ --------------------------- 10.7/33.5 MB 3.8 MB/s eta 0:00:06\n",
            "     ------------- -------------------------- 11.5/33.5 MB 3.8 MB/s eta 0:00:06\n",
            "     --------------- ------------------------ 12.6/33.5 MB 3.9 MB/s eta 0:00:06\n",
            "     --------------- ------------------------ 13.4/33.5 MB 3.9 MB/s eta 0:00:06\n",
            "     ----------------- ---------------------- 14.4/33.5 MB 3.9 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 15.2/33.5 MB 4.0 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 16.3/33.5 MB 4.0 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 17.0/33.5 MB 3.9 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 17.6/33.5 MB 3.9 MB/s eta 0:00:05\n",
            "     --------------------- ------------------ 18.4/33.5 MB 3.9 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 19.4/33.5 MB 3.9 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 20.2/33.5 MB 3.9 MB/s eta 0:00:04\n",
            "     ------------------------- -------------- 21.2/33.5 MB 3.9 MB/s eta 0:00:04\n",
            "     -------------------------- ------------- 22.0/33.5 MB 3.9 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 23.1/33.5 MB 4.0 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 24.1/33.5 MB 4.0 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 24.9/33.5 MB 4.0 MB/s eta 0:00:03\n",
            "     ------------------------------- -------- 26.0/33.5 MB 4.0 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 26.7/33.5 MB 4.0 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 27.8/33.5 MB 4.0 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 28.6/33.5 MB 4.0 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 29.4/33.5 MB 4.0 MB/s eta 0:00:02\n",
            "     ------------------------------------ --- 30.1/33.5 MB 4.0 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 31.2/33.5 MB 4.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 32.0/33.5 MB 4.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------  32.8/33.5 MB 4.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------  33.3/33.5 MB 4.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 33.5/33.5 MB 4.0 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Vector for 'machine':\n",
            " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
            "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
            "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
            " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
            "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
            "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
            "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
            " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
            " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
            " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
            "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
            "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
            "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
            "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
            " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
            "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
            "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
            " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
            "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
            "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
            " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
            "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
            "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
            " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
            " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
            " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
            " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
            " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
            "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
            " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
            "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
            "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
            "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
            " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
            " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
            "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
            " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
            " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
            " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
            "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
            "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
            " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
            " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
            "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
            " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
            "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
            " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
            "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
            " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
            "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n"
          ]
        }
      ],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example word vector\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medibot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
